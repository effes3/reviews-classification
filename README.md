# üìù Reviews Classification

**–ó–∞–¥–∞—á–∞**: –†–∞–∑–º–µ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **RuBERT**, **XLM-RoBERTa**, **CatBoost** –∏ **LoRA** –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö.

–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–∞—Ö –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å:
- **–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —á–µ—Ä–µ–∑ back-translation**.
- **–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞** –ø–æ—Ä—è–¥–∫–∞ –¥–µ—Å—è—Ç–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤.

–†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ `reviews-classification/data/final_submission_all.csv`

---

## üìå –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–µ—à–µ–Ω–∏—è

![–û–±—â–∞—è —Å—Ö–µ–º–∞ —Ä–∞–±–æ—Ç—ã](images/photo_2025-09-17_23-55-04.jpg)  
*–†–∏—Å. 1: –ú—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π —Ä–æ–∞–¥–º–∞–ø —Ä–µ—à–µ–Ω–∏–π,
—á—Ç–æ–±—ã –µ–≥–æ —Ä–∞—Å–º–º–æ—Ç—Ä–µ—Ç—å –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ, —Ç–æ —Å–∫–∞—á–∞–π—Ç–µ —Ñ–∞–π–ª* `reviews-classification/mind_roadmap.pdf`,
*—Ç–æ, —á—Ç–æ –æ–±–≤–µ–¥–µ–Ω–æ –∑–µ–ª–µ–Ω—ã–º ‚Äî –∫–æ–Ω–µ—á–Ω—ã–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –∏ –ø–∞–π–ø–ª–∞–π–Ω–∞*

---

## üè∑Ô∏è –ü—Ä–æ—Ü–µ—Å—Å —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö

### üîπ –≠—Ç–∞–ø 0: –ù–∞—á–∞–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
- **train_df**: 1717 –æ–±—Ä–∞–∑—Ü–æ–≤
- **val_df**: 100 –æ–±—Ä–∞–∑—Ü–æ–≤ (—Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞)

---

### üîπ –≠—Ç–∞–ø 1: –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å LLM (‚ùå –Ω–µ—É–¥–∞—á–Ω—ã–µ)
1. **Llama2 7B (zero-shot)**: –ù–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ Colab.
2. **Flan-T5-large (zero-shot)**: "–ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏".
3. **Helsinki-NLP/opus-mt-ru-en + Flan-T5-large**: –°–ª–∞–±–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞.

![–ù–µ—É–¥–∞—á–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏](images/photo_2025-09-18_00-05-27.jpg)  
*–†–∏—Å. 2: –ü–æ–ø—ã—Ç–∫–∏ 1‚Äì3 –Ω–∞ —Ä–æ–∞–¥–º–∞–ø–µ*

---

### üîπ –≠—Ç–∞–ø 2: –ü–µ—Ä–≤—ã–µ —Ä–∞–±–æ—á–∏–µ –ø–æ–¥—Ö–æ–¥—ã
- **Helsinki-NLP/opus-mt-ru-en + Flan-T5-base** ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Üí **CatBoost**.
- **Accuracy**: ~70% –Ω–∞ 50 —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö.
- ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞: –ø–æ—Ç–µ—Ä—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ.

---

### üîπ –≠—Ç–∞–ø 3: –ü—Ä–æ—Ä—ã–≤ —Å —Ä—É—Å—Å–∫–∏–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ ‚úÖ
- **DeepPavlov/rubert-base-cased** –∏ **ai-forever/sbert_large_nlu_ru** ‚Üí **CatBoost**.
- **Accuracy**: ~70‚Äì80%.
- –ü—Ä–æ–±–ª–µ–º–∞: –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ ‚Üí —Ä–µ—à–µ–Ω–æ —á–µ—Ä–µ–∑ `cleaning_datasets.ipynb`.

---

### üîπ –≠—Ç–∞–ø 4: –ê–∫—Ç–∏–≤–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∏ –∞–Ω—Å–∞–º–±–ª—å
**–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è RuBERT + CatBoost**:
1. –ü–µ—Ä–≤–∏—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ `valid`.
2. –†—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ —á–∞—Å—Ç–∏ `train`.
3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–æ–∏—Å–∫ —Å–ª–æ–∂–Ω—ã—Ö –∫–µ–π—Å–æ–≤.
4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞.
5. **–ê–Ω—Å–∞–º–±–ª—å RuBERT + XLM-RoBERTa (zero-shot)** ‚Üí —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫.

–ü—Ä–∏–º–µ—Ä –ª–æ–≥–∏–∫–∏ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è:
```python
if rubert == xlm:
    final = rubert
elif "—Ç–µ–∫—Å—Ç–∏–ª—å" in [rubert, xlm]:
    final = xlm if not agree else rubert
elif "—Ç–æ–≤–∞—Ä—ã –¥–ª—è –¥–µ—Ç–µ–π" in [rubert, xlm]:
    final = rubert
else:
    final = rubert
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: –†–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ `train`, `valid`, `test`.  
**–í—Ä–µ–º—è —Ä–∞–∑–º–µ—Ç–∫–∏** `test`: 13:02 (13 –º–∏–Ω—É—Ç, 2 —Å–µ–∫—É–Ω–¥—ã).

---

## üîÑ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ (Back-Translation)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è back-translation —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω—ã–π —è–∑—ã–∫ –∏–∑ `intermediate_languages`. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ `train` –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤.

```python
import random
import time
from deep_translator import GoogleTranslator

def augment_text(text, variations=3):
    """
    –§—É–Ω–∫—Ü–∏—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ back-translation.
    """
    augmented_texts = []
    intermediate_languages = ['en', 'de', 'fr', 'es', 'it']

    for i in range(variations):
        try:
            # –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —è–∑—ã–∫–∞
            intermediate_lang = random.choice(intermediate_languages)

            # –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —è–∑—ã–∫
            translated_intermediate = GoogleTranslator(
                source='ru',
                target=intermediate_lang
            ).translate(text)

            time.sleep(0.1)

            # –û–±—Ä–∞—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π
            back_translated = GoogleTranslator(
                source=intermediate_lang,
                target='ru'
            ).translate(translated_intermediate)

            augmented_texts.append(back_translated)
            time.sleep(0.2)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {e}")
            augmented_texts.append(text)

    return augmented_texts

train_df_augmented = pd.concat([train, augmented_df], ignore_index=True)
train_df_augmented.to_csv('train_split_augmented.csv', index=False)
```

–ü–æ–ª—É—á–µ–Ω–Ω—ã–π `train_df_augmented` –±—ã–ª —Ä–∞–∑–±–∏—Ç –Ω–∞ `train_df` –∏ `valid_df` —Å–æ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–π.

---

## ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å LoRA –Ω–∞ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **LoRA (Low-Rank Adaptation)** –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è **RuBERT** –Ω–∞ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

```python
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
import torch
import pickle
from sklearn.metrics import f1_score
import numpy as np

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ train –∏ validation –¥–∞–Ω–Ω—ã–µ
dataset = load_dataset('csv', data_files={
    'train': 'train_df_aug.csv',
    'validation': 'valid_df_aug.csv'
})

tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(dataset['train']['pseudo_label_ensemble_final'])
valid_labels = label_encoder.transform(dataset['validation']['pseudo_label_ensemble_final'])

with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

dataset['train'] = dataset['train'].add_column('label', train_labels)
dataset['validation'] = dataset['validation'].add_column('label', valid_labels)

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(['text', 'pseudo_label_ensemble_final'])
tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
classes = np.array(train_labels)
class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)
class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ LoRA
model = AutoModelForSequenceClassification.from_pretrained(
    'DeepPavlov/rubert-base-cased',
    num_labels=6,
    device_map='cuda'
)

lora_config = LoraConfig(
    r=32,
    lora_alpha=16,
    target_modules=['query', 'value'],
    lora_dropout=0.4,
    bias='none',
    task_type='SEQ_CLS'
)
model = get_peft_model(model, lora_config)

# –ö–∞—Å—Ç–æ–º–Ω—ã–π Trainer —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop('labels')
        outputs = model(**inputs)
        logits = outputs.logits
        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

# –§—É–Ω–∫—Ü–∏—è –º–µ—Ç—Ä–∏–∫ (F1 –ø–æ –∫–ª–∞—Å—Å–∞–º –∏ –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π)
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    weighted_f1 = f1_score(labels, predictions, average='weighted')
    per_class_f1 = f1_score(labels, predictions, average=None)
    class_names = label_encoder.classes_
    for cls, f1 in zip(class_names, per_class_f1):
        print(f"Class {cls}: F1 = {f1}")
        with open('metrics.txt', 'a') as f:
            f.write(f"Epoch {trainer.state.epoch}: Class {cls}: F1 = {f1}\n")
    with open('metrics.txt', 'a') as f:
        f.write(f"Epoch {trainer.state.epoch}: Weighted F1 = {weighted_f1}\n")
    print(f"Epoch {trainer.state.epoch}: Weighted F1 = {weighted_f1}")
    return {'weighted_f1': weighted_f1}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='./results_lora_1',
    num_train_epochs=20,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='eval_loss',
    greater_is_better=False,
    learning_rate=1e-4,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    fp16=True,
    seed=42
)

trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['validation'],
    compute_metrics=compute_metrics
)

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ WANDB –∏ –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
import os
os.environ["WANDB_DISABLED"] = "true"
import wandb
wandb.init(mode="disabled")

trainer.train()
```

---

## üîπ –ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ—Å—Ç–∞

```python
from peft import PeftModel
import torch
import pickle
import time
from tqdm import tqdm
from sklearn.metrics import f1_score
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')
checkpoint_path = './results_lora_1/checkpoint-1632'
model = AutoModelForSequenceClassification.from_pretrained(
    'DeepPavlov/rubert-base-cased',
    num_labels=6
)
model = PeftModel.from_pretrained(model, checkpoint_path)
model = model.to('cuda')

# –ó–∞–≥—Ä—É–∂–∞–µ–º label encoder
with open('label_encoder.pkl', 'rb') as f:
    label_encoder = pickle.load(f)

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
test_df = pd.read_csv('./FINAL_TEST.csv')
test_dataset = load_dataset('csv', data_files={'test': './FINAL_TEST.csv'})

def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)

tokenized_test = test_dataset['test'].map(tokenize_function, batched=True)
tokenized_test = tokenized_test.remove_columns(['text'])

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
model.eval()
predictions = []
start_time = time.time()

for i in tqdm(range(0, len(tokenized_test), 16)):
    batch = tokenized_test[i:i+16]
    inputs = {
        'input_ids': torch.tensor(batch['input_ids']).to('cuda'),
        'attention_mask': torch.tensor(batch['attention_mask']).to('cuda')
    }
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()
    predictions.extend(batch_preds)

# –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
elapsed_time = time.time() - start_time
avg_time_per_example = elapsed_time / len(test_df)
print(f"Average inference time per example: {avg_time_per_example:.3f} seconds")

# –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
predictions = label_encoder.inverse_transform(predictions)
predictions = ['—Ç–µ–∫—Å—Ç–∏–ª—å' if pred == '–æ–±—É–≤—å' else pred for pred in predictions]

# –ú–µ—Ç—Ä–∏–∫–∏
ground_truth = test_df['pseudo_label_ensemble_final'].values
weighted_f1 = f1_score(ground_truth, predictions, average='weighted')
per_class_f1 = f1_score(ground_truth, predictions, average=None)
class_names = sorted(test_df['pseudo_label_ensemble_final'].unique())
for cls, f1 in zip(class_names, per_class_f1):
    print(f"Class {cls}: F1 = {f1}")
print(f"Weighted F1-score on test set: {weighted_f1}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
test_df['predicted_label'] = predictions
test_df[['Unnamed: 0', 'text', 'pseudo_label_ensemble_final', 'predicted_label']].to_csv('test_predictions.csv', index=False)
with open('test_metrics.txt', 'w') as f:
    for cls, f1 in zip(class_names, per_class_f1):
        f.write(f"Class {cls}: F1 = {f1}\n")
    f.write(f"Weighted F1-score on test set: {weighted_f1}\n")
print("Predictions saved to test_predictions.csv")
print("Metrics saved to test_metrics.txt")
```

---

## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ–±—ä–µ–∫—Ç –≤** `test`: 0.031 —Å–µ–∫—É–Ω–¥—ã.
- **Weighted F1-score –Ω–∞ `test`**: 0.6984.
- –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ `test_metrics.txt` (–¥–ª—è `test`) –∏ `metrics.txt` (–¥–ª—è `LoRA`).
- –§–∏–Ω–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º .csv-—Ñ–æ—Ä–º–∞—Ç–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ `reviews-classification
/data/final_submission.csv`

---

## üõ†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è

- **data/**: `FINAL_TEST.csv`, `FINAL_TRAIN.csv`, `FINAL_VALID.csv`, `candidates.xlsx`, `candidates_2_labeled.xlsx`, `final_submission_all.csv`, `test.csv`, `train_1962.csv`, `valid_126.csv`
- **images/**: `photo_2025-09-17_23-55-04.jpg`, `photo_2025-09-18_00-05-27.jpg`.
- **src/**: `finalfinalfinal_combined.ipynb` (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –Ω–æ—É—Ç–±—É–∫–∏ `finalfinalfinal_all.ipynb` –∏ `cleaning_datasets.ipynb`), `finalfinalfinal_all.ipynb` (—Ä–∞–∑–º–µ—Ç–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ), `cleaning_datasets.ipynb` (—Ä–∞–±–æ—Ç–∞ —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏)
- **metrics/**: `metrics.txt`, `test_metrics.txt`.
- **LICENCE**: MIT License.
- **README.md**: –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è.
- **mind_roadmap.pdf**: –¥–µ—Ç–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —É–º—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞.

---

## ‚öôÔ∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–ø—É—Å–∫

**1. –ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π**:
   - –û—Ç–∫—Ä–æ–π—Ç–µ Google Colab –∏ —Å–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—ã–π –Ω–æ—É—Ç–±—É–∫.
   - –í—ã–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è:
     ```bash
     !git clone https://github.com/effes3/reviews-classification.git
     %cd reviews-classification
     ```

**2. –°–∫–∞—á–∞–π—Ç–µ –Ω–æ—É—Ç–±—É–∫**:
   - –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª `finalfinalfinal_all.ipynb` –∏–∑ –ø–∞–ø–∫–∏ `src`:
     ```bash
     !wget https://raw.githubusercontent.com/effes3/reviews-classification/main/src/finalfinalfinal_all.ipynb
     ```
   - –û—Ç–∫—Ä–æ–π—Ç–µ `finalfinalfinal_all.ipynb` –≤ Colab, –∑–∞–≥—Ä—É–∑–∏–≤ –µ–≥–æ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏–ª–∏ –≤—ã–ø–æ–ª–Ω–∏–≤:
     ```python
     from google.colab import files
     files.upload()  # –ó–∞–≥—Ä—É–∑–∏—Ç–µ finalfinalfinal_all.ipynb
     ```

**3. –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ**:
   - –ü–æ–¥–∫–ª—é—á–∏—Ç–µ Google –î–∏—Å–∫:
     ```python
     from google.colab import drive
     drive.mount('/content/drive')
     ```
   - –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É `NLP_CASE_FINAL` –Ω–∞ Google –î–∏—Å–∫–µ:
     ```bash
     !mkdir -p /content/drive/MyDrive/NLP_CASE_FINAL
     ```
   - –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–∞–ø–∫–∏ `data` —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (`FINAL_TEST.csv`, `FINAL_TRAIN.csv`, `FINAL_VALID.csv`, `candidates.xlsx`, `candidates_2_labeled.xlsx`, `test.csv`, `train_1962.csv`, `valid_126.csv`) –≤ `/content/drive/MyDrive/NLP_CASE_FINAL`. –î–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ `finalfinalfinal_combined.ipynb` –≤ —Ä–∞–±–æ—á—É—é –ø–∞–ø–∫—É –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª `test.csv`, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∫–µ–π—Å–∞. –ö–∞–∫ –∏—Ç–æ–≥, —ç—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Google –î–∏—Å–∫–∞ –∏–ª–∏ —Å –ø–æ–º–æ—â—å—é –∫–æ–º–∞–Ω–¥—ã:
     ```bash
     !cp data/* /content/drive/MyDrive/NLP_CASE_FINAL/
     ```
   - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤:
     ```bash
     !ls /content/drive/MyDrive/NLP_CASE_FINAL
     ```
   - –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø—É—Ç–∏ –≤ `finalfinalfinal_all.ipynb` —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ `/content/drive/MyDrive/NLP_CASE_FINAL/`, –Ω–∞–ø—Ä–∏–º–µ—Ä:
     ```python
     train = pd.read_csv('/content/drive/MyDrive/NLP_CASE_FINAL/FINAL_TRAIN.csv')
     valid = pd.read_csv('/content/drive/MyDrive/NLP_CASE_FINAL/FINAL_VALID.csv')
     test = pd.read_csv('/content/drive/MyDrive/NLP_CASE_FINAL/FINAL_TEST.csv')
     ```
**4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –Ω–æ—É—Ç–±—É–∫**:
   - –í—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫–∏ –≤ `finalfinalfinal_all.ipynb` –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Colab.
   - –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ GPU T4

---

## üë§ –ê–≤—Ç–æ—Ä

–ì—Ä–∏–≥–æ—Ä–∏–π, —Å—Ç—É–¥–µ–Ω—Ç 2 –∫—É—Ä—Å–∞ –ù–ò–£ –í–®–≠ (—Ñ–∞–∫—É–ª—å—Ç–µ—Ç —Ö–∏–º–∏–∏)  
Telegram: [@gsemak](https://t.me/gsemak),
GitHub: [effes3](https://github.com/effes3)  
–†–µ–∑—é–º–µ: [—Å—Å—ã–ª–∫–∞ –Ω–∞ —Ä–µ–∑—é–º–µ –Ω–∞ –Ø. –î–∏—Å–∫–µ](https://disk.360.yandex.ru/d/rIDufYfmGRIzaQ)

---

## üìú –õ–∏—Ü–µ–Ω–∑–∏—è

MIT License
