# üìù Reviews Classification  

**–ó–∞–¥–∞—á–∞**: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–≤–∞—Ä–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º  
**RuBERT, XLM-RoBERTa, CatBoost –∏ LoRA**.  

–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–∞—Ö –ø—Ä–∏–º–µ–Ω—è–ª–∞—Å—å:  
- **–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —á–µ—Ä–µ–∑ back-translation**,  
- **–∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –ø–æ—Ä—è–¥–∫–∞ –¥–µ—Å—è—Ç–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤**.  

---

## üìå –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–µ—à–µ–Ω–∏—è  

![–û–±—â–∞—è —Å—Ö–µ–º–∞ —Ä–∞–±–æ—Ç—ã](/images/photo_2025-09-17_23-55-04.jpg)  
*–†–∏—Å. 1: –ú—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π —Ä–æ–∞–¥–º–∞–ø —Ä–µ—à–µ–Ω–∏–π*  

---

## üè∑Ô∏è –ü—Ä–æ—Ü–µ—Å—Å —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö  

### üîπ –≠—Ç–∞–ø 0: –ù–∞—á–∞–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
- **train_df** ‚Üí 1717 samples  
- **val_df** ‚Üí 100 samples (—Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞)  

---

### üîπ –≠—Ç–∞–ø 1: –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å LLM (‚ùå –Ω–µ—É–¥–∞—á–Ω—ã–µ)
1. Llama2 7B (zero-shot) ‚Äî –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ Colab  
2. Flan-T5-large (zero-shot) ‚Äî "–≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏"  
3. Helsinki-NLP/opus-mt-ru-en + Flan-T5-large ‚Äî —Å–ª–∞–±–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞  

![–ù–µ—É–¥–∞—á–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏](images/photo_2025-09-18_00-05-27.jpg)  
*–†–∏—Å. 2: –ü–æ–ø—ã—Ç–∫–∏ 1‚Äì3 –Ω–∞ —Ä–æ–∞–¥–º–∞–ø–µ*  

---

### üîπ –≠—Ç–∞–ø 2: –ü–µ—Ä–≤—ã–µ —Ä–∞–±–æ—á–∏–µ –ø–æ–¥—Ö–æ–¥—ã  
- **Helsinki-NLP/opus-mt-ru-en + Flan-T5-base** ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Üí CatBoost  
- Accuracy ‚âà **70%** –Ω–∞ 50 —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö  
- ‚ö†Ô∏è –ü–æ—Ç–µ—Ä—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ  

---

### üîπ –≠—Ç–∞–ø 3: –ü—Ä–æ—Ä—ã–≤ —Å —Ä—É—Å—Å–∫–∏–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ ‚úÖ  
- **DeepPavlov/rubert-base-cased**  
- **ai-forever/sbert_large_nlu_ru** ‚Üí CatBoost  
- Accuracy ‚âà **70‚Äì80%**  
- –ü—Ä–æ–±–ª–µ–º–∞: –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ ‚Üí —Ä–µ—à–µ–Ω–æ —á–µ—Ä–µ–∑ `cleaning_datasets.ipynb`  

---

### üîπ –≠—Ç–∞–ø 4: –ê–∫—Ç–∏–≤–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∏ –∞–Ω—Å–∞–º–±–ª—å  

**–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è RuBERT + CatBoost**:  
1. –ü–µ—Ä–≤–∏—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ `valid`  
2. –†—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ —á–∞—Å—Ç–∏ `train`  
3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–æ–∏—Å–∫ —Å–ª–æ–∂–Ω—ã—Ö –∫–µ–π—Å–æ–≤  
4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞  
5. **–ê–Ω—Å–∞–º–±–ª—å RuBERT + XLM-RoBERTa (zero-shot)** ‚Üí —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫  

–ü—Ä–∏–º–µ—Ä –ª–æ–≥–∏–∫–∏ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è:  

```python
if rubert == xlm:
    final = rubert
elif "—Ç–µ–∫—Å—Ç–∏–ª—å" in [rubert, xlm]:
    final = xlm if not agree else rubert
elif "—Ç–æ–≤–∞—Ä—ã –¥–ª—è –¥–µ—Ç–µ–π" in [rubert, xlm]:
    final = rubert
else:
    final = rubert
```
–í –∏—Ç–æ–≥–µ –ø–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ `train`, `valid`, `test`. –í—Ä–µ–º—è —Ä–∞–∑–º–µ—Ç–∫–∏ `test` ‚Äî 13:02 (13 –º–∏–Ω—É—Ç, 2 —Å–µ–∫—É–Ω–¥—ã)

## üîÑ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ (Back-Translation)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è back-translation —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω—ã–π —è–∑—ã–∫ –∏–∑ `intermediate_languages`.  
–ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ `train` –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤.

```python
import random
import time
from deep_translator import GoogleTranslator

def augment_text(text, variations=3):
    """
    –§—É–Ω–∫—Ü–∏—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ back-translation.
    """
    augmented_texts = []
    intermediate_languages = ['en', 'de', 'fr', 'es', 'it']

    for i in range(variations):
        try:
            # –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —è–∑—ã–∫–∞
            intermediate_lang = random.choice(intermediate_languages)

            # –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —è–∑—ã–∫
            translated_intermediate = GoogleTranslator(
                source='ru',
                target=intermediate_lang
            ).translate(text)

            time.sleep(0.1)

            # –û–±—Ä–∞—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π
            back_translated = GoogleTranslator(
                source=intermediate_lang,
                target='ru'
            ).translate(translated_intermediate)

            augmented_texts.append(back_translated)
            time.sleep(0.2)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {e}")
            augmented_texts.append(text)

    return augmented_texts

train_df_augmented = pd.concat([train, augmented_df], ignore_index=True)
train_df_augmented.to_csv('train_split_augmented.csv', index=False)

```
–ü–æ–ª—É—á–µ–Ω–Ω—ã–π –Ω–æ–≤—ã–π train_df_augmented –±—ã–ª —Ä–∞–∑–±–∏—Ç –Ω–∞ `train_df` –∏ `valid_df` —Å–æ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞—Å—à—Ç–∞–±

## ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å LoRA –Ω–∞ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–ó–¥–µ—Å—å –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º **LoRA (Low-Rank Adaptation)** –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è RuBERT –Ω–∞ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

```python
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
import torch
import pickle
from sklearn.metrics import f1_score
import numpy as np

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ train –∏ validation –¥–∞–Ω–Ω—ã–µ
dataset = load_dataset('csv', data_files={
    'train': '/content/train_df_aug.csv', 
    'validation': '/content/valid_df_aug.csv'
})

tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(dataset['train']['pseudo_label_ensemble_final'])
valid_labels = label_encoder.transform(dataset['validation']['pseudo_label_ensemble_final'])

with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

dataset['train'] = dataset['train'].add_column('label', train_labels)
dataset['validation'] = dataset['validation'].add_column('label', valid_labels)

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(['text', 'pseudo_label_ensemble_final'])
tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
classes = np.array(train_labels)
class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)
class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ LoRA
model = AutoModelForSequenceClassification.from_pretrained(
    'DeepPavlov/rubert-base-cased',
    num_labels=6,
    device_map='cuda'
)

lora_config = LoraConfig(
    r=32,
    lora_alpha=16,
    target_modules=['query', 'value'],
    lora_dropout=0.4,
    bias='none',
    task_type='SEQ_CLS'
)
model = get_peft_model(model, lora_config)

# –ö–∞—Å—Ç–æ–º–Ω—ã–π Trainer —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop('labels')
        outputs = model(**inputs)
        logits = outputs.logits
        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

# –§—É–Ω–∫—Ü–∏—è –º–µ—Ç—Ä–∏–∫ (F1 –ø–æ –∫–ª–∞—Å—Å–∞–º –∏ –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π)
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    weighted_f1 = f1_score(labels, predictions, average='weighted')
    per_class_f1 = f1_score(labels, predictions, average=None)
    class_names = label_encoder.classes_
    for cls, f1 in zip(class_names, per_class_f1):
        print(f"Class {cls}: F1 = {f1}")
        with open('metrics.txt', 'a') as f:
            f.write(f"Epoch {trainer.state.epoch}: Class {cls}: F1 = {f1}\n")
    with open('metrics.txt', 'a') as f:
        f.write(f"Epoch {trainer.state.epoch}: Weighted F1 = {weighted_f1}\n")
    print(f"Epoch {trainer.state.epoch}: Weighted F1 = {weighted_f1}")
    return {'weighted_f1': weighted_f1}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='./results_lora_1',
    num_train_epochs=20,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='eval_loss',
    greater_is_better=False,
    learning_rate=1e-4,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    fp16=True,
    seed=42
)

trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['validation'],
    compute_metrics=compute_metrics
)

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ WANDB –∏ –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
import os
os.environ["WANDB_DISABLED"] = "true"
import wandb
wandb.init(mode="disabled")

trainer.train()
```

## üîπ –ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–µ—Å—Ç–∞

```python
from peft import PeftModel
import torch
import pickle
import time
from tqdm import tqdm
from sklearn.metrics import f1_score
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
tokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')
checkpoint_path = '/content/results_lora_1/checkpoint-1632'
model = AutoModelForSequenceClassification.from_pretrained(
    'DeepPavlov/rubert-base-cased',
    num_labels=6
)
model = PeftModel.from_pretrained(model, checkpoint_path)
model = model.to('cuda')

# –ó–∞–≥—Ä—É–∂–∞–µ–º label encoder
with open('label_encoder.pkl', 'rb') as f:
    label_encoder = pickle.load(f)

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
test_df = pd.read_csv('/content/drive/MyDrive/NLP_CASE_FINAL/FINAL_TEST.csv')
test_dataset = load_dataset('csv', data_files={'test': '/content/drive/MyDrive/NLP_CASE_FINAL/FINAL_TEST.csv'})

def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)

tokenized_test = test_dataset['test'].map(tokenize_function, batched=True)
tokenized_test = tokenized_test.remove_columns(['text'])

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
model.eval()
predictions = []
start_time = time.time()

for i in tqdm(range(0, len(tokenized_test), 16)):
    batch = tokenized_test[i:i+16]
    inputs = {
        'input_ids': torch.tensor(batch['input_ids']).to('cuda'),
        'attention_mask': torch.tensor(batch['attention_mask']).to('cuda')
    }
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()
    predictions.extend(batch_preds)

# –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
elapsed_time = time.time() - start_time
avg_time_per_example = elapsed_time / len(test_df)
print(f"Average inference time per example: {avg_time_per_example:.3f} seconds")

# –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
predictions = label_encoder.inverse_transform(predictions)
predictions = ['—Ç–µ–∫—Å—Ç–∏–ª—å' if pred == '–æ–±—É–≤—å' else pred for pred in predictions]

# –ú–µ—Ç—Ä–∏–∫–∏
ground_truth = test_df['pseudo_label_ensemble_final'].values
weighted_f1 = f1_score(ground_truth, predictions, average='weighted')
per_class_f1 = f1_score(ground_truth, predictions, average=None)
class_names = sorted(test_df['pseudo_label_ensemble_final'].unique())
for cls, f1 in zip(class_names, per_class_f1):
    print(f"Class {cls}: F1 = {f1}")
print(f"Weighted F1-score on test set: {weighted_f1}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
test_df['predicted_label'] = predictions
test_df[['Unnamed: 0', 'text', 'pseudo_label_ensemble_final', 'predicted_label']].to_csv('test_predictions.csv', index=False)
with open('test_metrics.txt', 'w') as f:
    for cls, f1 in zip(class_names, per_class_f1):
        f.write(f"Class {cls}: F1 = {f1}\n")
    f.write(f"Weighted F1-score on test set: {weighted_f1}\n")
print("Predictions saved to test_predictions.csv")
print("Metrics saved to test_metrics.txt")
```

## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞ –æ–±—ä–µ–∫—Ç–∞ –≤ `test`: 0.031 seconds
- Weighted F1-score –Ω–∞ `test`: 0.6983976359503813
- –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ `test_metrics.txt` –¥–ª—è `test` –∏ –≤ `metrics.txt` –¥–ª—è `LoRA`
